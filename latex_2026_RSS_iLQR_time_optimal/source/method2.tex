
\subsection{Augmented Dynamics and Cost}
\label{sec:augmented_dynamics}

Standard DDP/iLQR~\cite{li2004ilqr} solves nonlinear OC by iteratively approximating them as linear-quadratic sub-problems.
Linearization introduces affine terms, which break the standard pure-quadratic LQR form ($x^\top P x$) required by our LFT-based method as aforementioned.
To address this, we perform a transformation including (1) approximation of the dynamics and cost function, (2) quadratization via completing the square, and (3) state augmentation.

\subsubsection{Approximation}
Given a nominal trajectory $(\bar{x}_k, \bar{u}_k)$, we define the deviations $\delta x_k := x_k - \bar{x}_k$ and $\delta u_k := u_k - \bar{u}_k$. The linearized dynamics are:
\begin{equation}
    \delta x_{k+1} = A_k \delta x_k + B_k \delta u_k + a_k,
\end{equation}
where $A_k = \nabla_x f(\bar{x}_k, \bar{u}_k)$, $B_k = \nabla_u f(\bar{x}_k, \bar{u}_k)$, and $a_k = f(\bar{x}_k, \bar{u}_k) - \bar{x}_{k+1}$.

The stage cost is Taylor expanded to the second order:
\begin{equation}
\begin{aligned}
\ell_k &\approx \ell(\bar{x}_k, \bar{u}_k) + w + \ell_{x,k}^\top \delta x_k + \ell_{u,k}^\top \delta u_k \\
&\quad + \tfrac{1}{2} \delta x_k^\top \ell_{xx,k} \delta x_k + \delta x_k^\top \ell_{xu,k} \delta u_k \\
&\quad + \tfrac{1}{2} \delta u_k^\top \ell_{uu,k} \delta u_k,
\end{aligned}
\end{equation}
which are partial derivatives evaluated at $(\bar{x}_k, \bar{u}_k)$, and the time penalty $w$ is included as a constant term.

\subsubsection{Quadratization}
The cross-term $\delta x_k^\top \ell_{xu,k} \delta u_k$ prevents direct application of standard LQR methods. To eliminate it, we complete the square for the control terms.
We first group all terms dependent on $\delta u_k$:
\[
\tfrac{1}{2} \delta u_k^\top \ell_{uu,k} \delta u_k + \delta u_k^\top (\ell_{ux,k} \delta x_k + \ell_{u,k}).
\]
Recalling the identity $\frac{1}{2}v^\top R v = \frac{1}{2}(u + R^{-1}b)^\top R (u + R^{-1}b) = \frac{1}{2}u^\top R u + u^\top b + \text{const.}$, we can match terms to define the new control variable $v_k$:
\begin{equation}\label{eq:new_control}
    v_k := \delta u_k + \ell_{uu,k}^{-1}(\ell_{ux,k} \delta x_k + \ell_{u,k}).
\end{equation}
Substituting $v_k$ back into the cost function allows us to express the stage cost in a decoupled form. The expansion is:
\begin{equation}
\begin{aligned}
\ell_k &\approx \tfrac{1}{2} \delta x_k^\top (\ell_{xx,k} - \ell_{xu,k} \ell_{uu,k}^{-1} \ell_{ux,k}) \delta x_k \\
&\quad + (\ell_{x,k} - \ell_{xu,k} \ell_{uu,k}^{-1} \ell_{u,k})^\top \delta x_k \\
&\quad + \tfrac{1}{2} v_k^\top \ell_{uu,k} v_k \\
&\quad + \left( \ell(\bar{x}_k, \bar{u}_k) + w - \tfrac{1}{2} \ell_{u,k}^\top \ell_{uu,k}^{-1} \ell_{u,k} \right).
\end{aligned}
\end{equation}

\subsubsection{Augmentation}
Even after using the new control variable, the dynamics and cost still contain state-dependent affine terms. We eliminate these by lifting the system into a homogeneous coordinate space via the augmented state
$z_k = [\delta x_k,\; 1]^T$.
This allows us to absorb all linear and constant terms into the quadratic form.

\paragraph{Augmented Dynamics}
Substituting the new control variable $v_k$ (Eq.~\ref{eq:new_control}) into the dynamics yields the explicit update equation:
\begin{equation}
\begin{aligned}
    \delta x_{k+1} &= \left( A_k - B_k \ell_{uu,k}^{-1} \ell_{ux,k} \right) \delta x_k + B_k v_k \\
    &\quad + \left( a_k - B_k \ell_{uu,k}^{-1} \ell_{u,k} \right).
\end{aligned}
\end{equation}
We then rewrite it based on $z_k$  and get the new augmented system dynamics:
\begin{equation}
    % \boxed{
    z_{k+1} = A_k^{\text{aug}} z_k + B_k^{\text{aug}} v_k,
    % }
\end{equation}
\begin{equation}\label{eq:aug_dynamics}
    A_k^{\text{aug}} = \begin{bmatrix}
    A_k - B_k \ell_{uu,k}^{-1} \ell_{ux,k} & a_k - B_k \ell_{uu,k}^{-1} \ell_{u,k} \\
    0 & 1
    \end{bmatrix},
\end{equation}
\begin{equation}
    B_k^{\text{aug}} = \begin{bmatrix}
    B_k \\ 0
    \end{bmatrix}.
\end{equation}

\paragraph{Augmented Cost}
Similarly, we define notations for the modified cost terms:
\[
\tilde{Q}_k = \ell_{xx,k} - \ell_{xu,k} \ell_{uu,k}^{-1} \ell_{ux,k}, \quad
\tilde{q}_k = \ell_{x,k} - \ell_{xu,k} \ell_{uu,k}^{-1} \ell_{u,k}.
\]
Then the augmented cost matrix becomes:
\begin{equation}\label{eq:aug_cost_matrices}
Q_k^{\text{aug}} = \begin{bmatrix}
\tilde{Q}_k & \tilde{q}_k \\
\tilde{q}_k^\top & 2\left(\ell(\bar{x}_k, \bar{u}_k) + w - \tfrac{1}{2} \ell_{u,k}^\top \ell_{uu,k}^{-1} \ell_{u,k}\right)
\end{bmatrix}.
\end{equation}
\[
R_k = \ell_{uu,k}.
\]

The stage cost in augmented form is quadratic:
\begin{equation}\label{eq:aug_stage_cost}
\ell_k \approx \tfrac{1}{2} z_k^\top Q_k^{\text{aug}} z_k + \tfrac{1}{2} v_k^\top R_k v_k
\end{equation}

Similarly, the terminal cost matrix is:
\begin{equation}\label{eq:aug_terminal_cost}
Q_N^{\text{aug}} = \begin{bmatrix}
\phi_{xx,N} & \phi_{x,N} \\
\phi_{x,N}^\top & 2\phi(\bar{x}_N)
\end{bmatrix}.
\end{equation}

% ------------------------------------------------------------------





\begin{algorithm}[tb]
\SetAlgoLined
\DontPrintSemicolon
\caption{\abbrAlgg Algorithm}
\label{alg:time-optimal-ilqr}
\small
\KwIn{Dynamics $f$, costs $\ell, \phi$, initial state $x_0$, initial controls $U$, horizon bounds $[T_{\min}, T_{\max}]$, time penalty $w$, $N = T_{\max}$ }
\KwOut{Optimal trajectory and controls}

\Repeat{convergence}{
    \tcp{Step 1: Linearization and augmentation}
    Rollout $X$ using $f$ and $U$\;
    \For{$k = 0$ \KwTo $N-1$}{
        Compute $A_k, B_k$ at $(\bar{x}_k, \bar{u}_k)$\\
        Compute derivatives: $\ell_{x,k}, \ell_{u,k}, \ell_{xx,k}, \ell_{ux,k}, \ell_{uu,k}$\\
        Compute $A_k^{\text{aug}}, B_k^{\text{aug}}, Q_k^{\text{aug}}, R_k$ via Eqs.~\eqref{eq:aug_dynamics}-\eqref{eq:aug_cost_matrices}\;
    }
    Build $Q_T^{\text{aug}}$ from $\phi$ at $\bar{x}_T$\;
    
    \BlankLine
    \tcp{Step 2: Horizon selection}
    $J \gets$ \abbrAlg$(A^{\text{aug}}, B^{\text{aug}}, Q^{\text{aug}}, R, z_0, Q_T^{\text{aug}}, N)$\\
    $T^* \gets \arg\min_{t \in [T_{\min}, T_{\max}]} J[t]$\;
    
    \BlankLine
    \tcp{Step 3: Backward pass on $[0, T^*-1]$}
    % Initialize: 
    
    $V_{xx}[T^*] \gets \phi_{xx}$, $V_x[T^*] \gets \phi_x$, $V_0[T^*] \gets \phi(\bar{x}_{T^*})$\\
    \For{$k = T^*-1, T^*-2, \cdots,0$}{
        \tcp{Compute derivatives (DDP terms in parentheses, iLQR ignores them)}
        $Q_x  \gets \ell_{x,k} + A_k^\top V_{x,k+1}\; \bigl(+\, A_k^\top V_{xx,k+1}\, a_k \bigr)$\\
        $Q_u  \gets \ell_{u,k} + B_k^\top V_{x,k+1}\; \bigl(+\, B_k^\top V_{xx,k+1}\, a_k \bigr)$\\        
        $Q_{xx} \gets \ell_{xx,k} + A_k^\top V_{xx,k+1} A_k\;
        \bigl(+\, \sum_{i=1}^{n_x} V_{x,k+1}^{(i)}\, f_{xx,k}^{(i)} \bigr)$\\    
        $Q_{ux} \gets \ell_{ux,k} + B_k^\top V_{xx,k+1} A_k\;
        \bigl(+\, \sum_{i=1}^{n_x} V_{x,k+1}^{(i)}\, f_{ux,k}^{(i)} \bigr)$\\
        $Q_{uu} \gets \ell_{uu,k} + B_k^\top V_{xx,k+1} B_k\;
        \bigl(+\, \sum_{i=1}^{n_x} V_{x,k+1}^{(i)}\, f_{uu,k}^{(i)} \bigr)$\\
        $Q_{xu} \gets Q_{ux}^\top$\\
        $Q_{uu} \gets Q_{uu} + \lambda I$ \tcp*{LM regularization}

        
        \tcp{Compute gains}
        $\kappa_k \gets -Q_{uu}^{-1} Q_u$, $K_k \gets -Q_{uu}^{-1} Q_{ux}$
        
        \tcp{Update value function}
        $V_{xx,k} \gets Q_{xx} - Q_{ux}^\top Q_{uu}^{-1} Q_{ux}$\\
        $V_{x,k} \gets Q_x - Q_{ux}^\top Q_{uu}^{-1} Q_u$\\
    $V_{0,k} \gets V_{0,k+1} + \ell(\bar{x}_k,\bar{u}_k) + w
          + \kappa_k^\top Q_u + \tfrac{1}{2}\kappa_k^\top Q_{uu}\kappa_k$

    }
    
    \BlankLine
    \tcp{Step 4: Forward rollout with line search}
    \For{$\alpha \in I_{\alpha}$}{
        $x_{\text{new}}[0] \gets x_0$\\
        \For{$k = 0$ \KwTo $T^*-1$}{
            $\delta x \gets x_{\text{new}}[k] - \bar{x}_k$\\
            $\delta u \gets K_k \cdot \delta x + \alpha \cdot \kappa_k$\\
            $u_{\text{new}}[k] \gets \bar{u}_k + \delta u$\\
            $x_{\text{new}}[k+1] \gets f(x_{\text{new}}[k], u_{\text{new}}[k])$\;
        }
        \If{TrueCost$(x_{\text{new}}, u_{\text{new}}, T^*) <$ TrueCost$(\bar{x}, \bar{u}, T^*)$}{
            Accept trajectory; \textbf{break}\;
        }
    }
    \If{not accepted}{$\lambda *= 10$ \tcp*{Increase LM regularization}}
    \Else{Update nominal $(X, U)$}
}
\end{algorithm}

\subsection{\abbrAlgg Algorithm}

We now present the complete \abbrAlgg Algorithm. With the augmented state space derived in Section~\ref{sec:augmented_dynamics}, the iterative optimization can be described in four steps (Alg.~\ref{alg:time-optimal-ilqr}).

\subsubsection{Step 1: Linearization and Augmentation}
The iteration begins by linearizing the nonlinear dynamics and quadratizing the cost function around the current nominal trajectory $(\bar{x}, \bar{u})$ as mentioned in Sec.~\ref{sec:augmented_dynamics}, which yields the augmented system matrices $A_k^{\text{aug}}, B_k^{\text{aug}}$ (Eq.~\ref{eq:aug_dynamics}) and the augmented cost matrices $Q_k^{\text{aug}}, R_k$ (Eq.~\ref{eq:aug_stage_cost}).

% This transformation yields a sequence of linear-quadratic parameters $\left\{ (A_k^{\text{aug}}, B_k^{\text{aug}}, Q_k^{\text{aug}}, R_k) \right\}_{k=0}^{N-1}$ for the entire horizon. 
% This sequence serves as the direct input for the \abbrAlg Algorithm in the next step.

\subsubsection{Step 2: Horizon Selection}
With the system $\left\{ (A_k^{\text{aug}}, B_k^{\text{aug}}, Q_k^{\text{aug}}, R_k) \right\}_{k=0}^{N-1}$ in a standard linear-quadratic form, we can now find an optimal horizon $T^*$ by calling the aforementioned \abbrAlg.
% solve the horizon selection sub-problem via \abbrAlg Algorithm.
This step identifies the optimal stopping time $T^*$ without requiring a full backward pass for each candidate horizon.

% Unlike standard iLQR which assumes a fixed $T$, we must identify the optimal time-horizon $T^*$ that minimizes the  cost.

% Using the \abbrAlg Algorithm (Method 1), we efficiently evaluate the optimal cost-to-go $J_t$ for every candidate horizon $t \in [T_{\min}, T_{\max}]$ simultaneously:
% \begin{equation}
% T^* = \arg\min_{t \in [T_{\min}, T_{\max}]} \left( \tfrac{1}{2} z_0^\top (\tilde{P}_0^{(t)})^{-1} z_0 \right).
% \end{equation}
% This step identifies the optimal stopping time $T^*$ without requiring a full backward pass for each candidate.

\subsubsection{Step 3: Truncated Backward Pass}
Once $T^*$ is known, we perform the \textit{truncated} backward pass, which is executed only on the interval $[0, T^*-1]$, rather than the maximum possible horizon $N$.
Specifically, we initialize the value function at $T^*$ using the terminal cost $\phi(\bar{x}_{T^*})$. Then, proceeding backward from $k = T^*-1$ to $0$, we compute the derivatives of $Q$-matrices and extract the feedback gains as in regular DDP:
\begin{equation}
    K_k = -Q_{uu,k}^{-1} Q_{ux,k}, \quad \kappa_k = -Q_{uu,k}^{-1} Q_{u,k}.
\end{equation}
This yields the modification on the current control $\delta u_k = K_k \delta x_k + \kappa_k$ for the selected horizon $T^*$.

\subsubsection{Step 4: Forward Rollout and Update}
Finally, we apply the computed gains to the original nonlinear system $x_{k+1} = f(x_k, u_k)$ to generate a new trajectory.
We perform a line search on the step size $\alpha \in I_{\alpha}$ to ensure convergence, where $I_{\alpha}$ is a user-specified set of possible step sizes.
To generate the new trajectory, we compute the new controls as follows.
\begin{equation}
u_k^{\text{new}} = \bar{u}_k + K_k(x_k^{\text{new}} - \bar{x}_k) + \alpha \kappa_k.
\end{equation}
Based on the computed new controls, a new state trajectory can be rolled out. 
This rollout is terminated at time step $T^*$.
If the total nonlinear cost of the trajectory decreases, the new trajectory is accepted as the nominal trajectory for the next iteration; otherwise, the regularization parameter is increased similarly to \cite{todorov2005ilqg}.

% \vspace{0.5em}


% \begin{remark}
% iLQR/DDP solves the fixed-horizon nonlinear OC by alternating linearization/quadratization, a backward pass, and a forward rollout~\cite{jacobson1970ddp,murray1984ddpnewton,li2004ilqr}.
% For horizon-optimal nonlinear OC, the literature~\cite{stachowicz2021ohmp} accelerates horizon selection by evaluating horizons approximately using a backward pass around a nominal horizon.
% This nominal horizon is computed by the one-pass approach that approximates the system as a time-invariant LQR.
% ???
% , which can be inaccurate when the local models change across horizons/iterations~\cite{stachowicz2021ohmp}.
% HOP-DDP instead performs exact horizon querying for the current LQ approximation via the augmented-state reduction and HOP-LQR, then runs the backward pass only on $[0,T^*-1]$.
% \end{remark}